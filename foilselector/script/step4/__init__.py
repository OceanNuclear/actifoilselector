__all__= ["rank_foils"]
def run():
    from .rank_foils import *
    def tprint(*msg):
        print("\n[t=+{:2.2f}s]".format(time.time()-prog_start_time), *msg)

    prog_start_time = time.time()
    ############################# get the relevant data from files. ####################################
    assert os.path.exists(os.path.join(sys.argv[-1], 'gs.csv')), "Output directory must already have gs.csv"
    gs = pd.read_csv(os.path.join(sys.argv[-1], 'gs.csv')).values
    APRIORI_FLUX, APRIORI_FLUENCE = get_apriori(sys.argv[-1], get_parameters_json(sys.argv[-1])["IRRADIATION_DURATION"])

    _msg = "A 'counts.csv' file must exist (generated by GetReactionRates.py) at the target directory listed by the last argv argument."
    assert os.path.exists(os.path.join(sys.argv[-1], "counts.csv")), _msg
    _msg = "A 'microscopic_xs.csv' file must exist (generated by ReadData.py) at the target directory listed by the last argv argument."
    assert os.path.exists(os.path.join(sys.argv[-1], "microscopic_xs.csv")), _msg
    tprint("Reading the per reactant atom/ per primary product information from 'counts.csv'...")
    population = pd.read_csv(os.path.join(sys.argv[-1], "counts.csv"), index_col=[0])
    population = unserialize_pd_DataFrame(population)
    # get the list of reactants for the reactions too
    reactants = ary([parent_product_mt.split("-")[0] for parent_product_mt in population.index])
    tprint("Reading the microscopic cross-section...")
    full_sigma_matrix = pd.read_csv(os.path.join(sys.argv[-1], "microscopic_xs.csv"), index_col=[0])
    full_sigma_matrix = full_sigma_matrix.loc[population.index]
    tprint("Getting the list of all available materials and their physical parameters...")
    # phy_prop_file_path = PHYSICAL_PROP_FILE
    phy_prop_file_path = ".physical_parameters/elemental_frac_isotopic_frac_physical_short.csv"
    phy_prop = get_physical_property(phy_prop_file_path)
    if LIST_PRICE:
        from misc_library import get_price_df, get_price_from_price_df, PRICE_FILE
        price_df = get_price_df()

    ###################### Give alias to functions so that they're easier to use/recall. ###############
    get_sensitivity = lambda r, cnts: scalar_curvature(r, np.diag(1/cnts), APRIORI_FLUENCE)
    get_specificity_cnt = lambda r: np.clip(np.divide(*r.shape), 0, 1)
    get_specificity_lin = lambda r: confusion_matrix_correctly_identified_diagonal(r, APRIORI_FLUENCE, confusion_matrix_linear)
    get_specificity_log = lambda r: confusion_matrix_correctly_identified_diagonal(r, APRIORI_FLUENCE, confusion_matrix_log)
    # There should be one more

    tprint("Stripping the uncertainties from the 'counts measured by detector per reactant atom' column...")
    nominal_population = pd.DataFrame( nominal_values(population), index=population.index, columns=population.columns)

    ############################### create the list of candidate materials #############################
    foil_candidates = []
    tprint(f"Creating the foils, using count_threshold = {COUNT_THRESHOLD}")
    material_progress_bar = tqdm(phy_prop.iterrows(), total=phy_prop.shape[0])
    for material_name, material in material_progress_bar:
        material_progress_bar.set_description(material.name)
        # magic numbers: procedures specific to this very physical_property_file that I'm using.
        melting_point = material[2]
        total_number_density = material[4] 
        if np.isnan(total_number_density):
            continue # skip the cases with undefined densities
        partial_number_density = material[5:257] * total_number_density
        # end of magic numbers.
        # partial_number_density = a pd.Series showing all the isotopes with non-zero partial number density..
        parent_product_mts, number_densities = pd.Index([]), []
        for species, species_density in partial_number_density[partial_number_density>0].iteritems():
            matching_reactions = nominal_population.index[ reactants==species ]
            number_densities.extend([ species_density for _ in matching_reactions ])
            parent_product_mts = parent_product_mts.append( matching_reactions )

        number_densities = pd.Series(number_densities, index=parent_product_mts, name="number density")
        population_slice = nominal_population.loc[parent_product_mts]

        # create a foil
        foil = Foil(
            material_name,
            number_densities,
            full_sigma_matrix.loc[parent_product_mts],
            population_slice["production of primary product per reactant atom"],
            population_slice["final counts measured by detector PPP"],
            population_slice[["activity before measurement PPP", "activity after measurement PPP"]],
            population_slice["max microscopic cross-section"],
            # area=FOIL_AREA,
            melting_point=melting_point,
            # price=np.nan, # this is the default
            )
        foil_candidates.append(foil)

    tprint("Fixing the foil dimensions and then turning these foils into dictionaries...")
    foil_candidates_dict = {}
    for foil in foil_candidates:
        merged_foil = foil.merge_duplicates_and_filter() # merge the duplicate reactions,
        if len(merged_foil.counts)==0:
            continue # skip the foils that doesn't give any detectable reactions.
        if LIST_PRICE:
            try:
                price = get_price_from_price_df(price_df, merged_foil.material_name, merged_foil.thickness)
            except KeyError: # no price info for that kind of material.
                price = np.nan
            merged_foil.price = price
        foil_candidates_dict[merged_foil.material_name] = merged_foil

    # reorder them
    tprint("Calculating the specificity and sensitivity of each foil")
    individual_sensitivity = OrderedDict((f_name, get_sensitivity(foil.response_per_unit_flux.values, foil.counts.values)) for f_name, foil in foil_candidates_dict.items())
    individual_specificity_cnt = OrderedDict((f_name, get_specificity_cnt(foil.response_per_unit_flux.values)) for f_name, foil in foil_candidates_dict.items())
    individual_specificity_lin = OrderedDict((f_name, get_specificity_lin(foil.response_per_unit_flux.values)) for f_name, foil in foil_candidates_dict.items())
    individual_specificity_log = OrderedDict((f_name, get_specificity_log(foil.response_per_unit_flux.values)) for f_name, foil in foil_candidates_dict.items())
    num_reactions = OrderedDict((f_name, foil.response_per_unit_flux.shape[0]) for f_name, foil in foil_candidates_dict.items())
    thicknesses = OrderedDict((f_name, foil.thickness) for f_name, foil in foil_candidates_dict.items())
    foil_data_df = pd.DataFrame([thicknesses, num_reactions, individual_sensitivity, individual_specificity_cnt, individual_specificity_lin, individual_specificity_log],
        index=["thickness", "number of reactions", "individual sensitivity", "individual specificity (count)", "individual specificity (linear)", "individual specificity (log)"]).T
    foil_data_df["number of reactions"] = foil_data_df["number of reactions"].astype("int")

    # filter out unwanted foils by interacting with user.
    excluded_materials = ""
    while True:
        mask = ary([index not in excluded_materials.split(",") for index in foil_data_df.index])
        with pd.option_context("display.max_rows", None):
            print(foil_data_df[mask])
        if "y" in input("Are you satisfied with this selection of {} candidate materials? (y/n)".format(mask.sum())).lower():
            break
        print("\n".join(foil_candidates_dict.keys()))
        print("If not, please enter the names of the foils that you would like to remove, delimited by ',':")
        excluded_materials = input()

    # create reduced_foil_set which exclude all unwanted foils.
    for name in list(foil_candidates_dict.keys()):
        if name in excluded_materials.split(","):
            foil_candidates_dict.pop(name)
            individual_sensitivity.pop(name)
            individual_specificity_cnt.pop(name)
            individual_specificity_lin.pop(name)
            individual_specificity_log.pop(name)
            num_reactions.pop(name)
            thicknesses.pop(name)
            foil_data_df.drop(name, inplace=True)
    reduced_foil_set = FoilSet(*list(foil_candidates_dict.values()))

    # save the reduced_foil_set's parameters
    saved_foil_params = {}
    for foil_name in reduced_foil_set.material_name:
        foil = foil_candidates_dict[foil_name] # alias to make the following 3 lines shorter
        saved_foil_params[foil_name] = {
        "thickness (cm)": foil.thickness,
        "area (cm^2)" : foil.area,
        "number density (cm^-3)": foil.number_densities,
        "melting point": foil.melting_point,
        "price" : foil.price,
        "total counts at thickness = {}cm".format(foil.thickness): foil.counts.to_dict(),
        "specificity (count)" : individual_specificity_cnt[foil.material_name],
        "specificity (linear)" : individual_specificity_lin[foil.material_name],
        "specificity (log)" : individual_specificity_log[foil.material_name],
        "sensitivity" : individual_sensitivity[foil.material_name],
        }

    # slight sorting to make it more user-friendly/ reader-friendly:
    # put the highest number of reaction ones first;
    # if there are two elements with the highest number of reactions,
    # then sort by the total counts
    def get_num_reactions_and_counts_from_dict(items):
        key, val = items
        total_counts_key = [i for i in val.keys() if i.startswith("total counts")][0]
        total_counts = val[total_counts_key]
        return len(total_counts), sum(total_counts.values())
    saved_foil_params = OrderedDict(sorted(saved_foil_params.items(),
                                    key=get_num_reactions_and_counts_from_dict,
                                    reverse=True)
                                    )
    with open(os.path.join(sys.argv[-1], "possible_selection.json"), "w") as j:
        # save it in a way that makes the json file human readable.
        j.write(json.dumps(saved_foil_params, indent=4))
    sys.exit()

    # functions to perform optimization
    @functools.lru_cache(maxsize=2**31)
    def get_specificity_from_names(*names):
        foil_set = chain_sum([foil_candidates_dict[name] for name in names])
        return get_specificity_lin(foil_set.response_per_unit_flux)

    def get_specificity_from_unsorted_names(*names):
        return get_specificity_from_names(*sorted(names))

    def negative_get_specificity_from_unsorted_names(*names):
        return -get_specificity_from_unsorted_names(*names)

    # calculate the specificity and senitivity for each foilset, and then rank them.
    if SCATTERPLOT:=False:
        specificity, sensitivity = {}, {}
        # generate a complete set of solution
        num_foils_chosen = 3 # honestly, 71 choose 3 is a huge number so I really dont' want to make it any bigger.
        # It typically runs through 67 combinations per second.
        num_combinations = int(fac(len(foil_candidates_dict)) / (fac(len(foil_candidates_dict)-num_foils_chosen) * fac(num_foils_chosen)))
        print("Creating the scatter plot of sensitivity vs specificity, one data point for each of the {} choose {} = {} combination of foils:".format(len(foil_candidates_dict),
                                                                                                                                                        num_foils_chosen, num_combinations))
        for f_set in tqdm(itertools.combinations(foil_candidates_dict.values(), num_foils_chosen), total=num_combinations):
            f_set = chain_sum(f_set)
            f_set_name = get_foilset_condensed_name(f_set.material_name)
            specificity[f_set_name] = get_specificity_lin(f_set.response_per_unit_flux)
            sensitivity[f_set_name] = get_sensitivity(f_set.response_per_unit_flux)
        
        metrics = pd.DataFrame([sensitivity, specificity], ["sensitivity", "specificity"]).T
        plt.scatter(*metrics.values.T, marker='x', alpha=0.8)
        plt.xlim( 0.8*metrics["sensitivity"].min(), 1.2*metrics["sensitivity"].max() )
        plt.xlabel("sensitivity (sum of diagonal of the curvature matrix)")
        plt.ylabel("specificity")
        plt.title("{} foils choose {} = {} combinations possible ".format(len(foil_candidates_dict), num_foils_chosen, num_combinations))
        plt.show()

    num_foils_chosen = 6
    num_foils_chosen_list = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
    # selection process: building the best combination lists.
    if BUILD_SENSITIVITY_LIST:=False:
        num_top_sensitivity = 5000
        print("Trying to build the list of top sensitivity combinations, which consists of picking the top {} for {} different num_foils_chosen:".format(num_top_sensitivity, len(num_foils_chosen_list)))
        for num_foils_chosen in tqdm(num_foils_chosen_list):
            top_sensitivity_combo = top_n_sums_of_dict(individual_sensitivity, num_foils_chosen, num_top_sensitivity)
            # top_sensitivity_combo_names = ["".join(k) for k in top_sensitivity_combo.keys()]
            saving_dict = OrderedDict()
            for key, val in descending_odict(top_sensitivity_combo).items():
                str_key = "-".join(key)
                foil_set = chain_sum([foil_candidates_dict[name] for name in key])
                saving_dict[str_key] = {"sensitivity-specificity":(val, get_specificity_lin(foil_set.response_per_unit_flux)), "min melting point": foil_set.get_min_melting_point()}
                if LIST_PRICE:
                    saving_dict[str_key]["price"] = foil_set.get_price_expr()

            with open(os.path.join(sys.argv[-1], str(num_foils_chosen).zfill(2)+"_sensitivity_combo.json"), "w") as j:
                j.write(json.dumps(saving_dict, indent=4))
                
    if BUILD_SPECIFICITY_LIST:=True:
        """
        policy = 0.05
        def custom_policy(odict):
            val = odict.values()
            n = np.clip(int(len(val)//12), 1, None) # choose a fixed fraction of it
            # n = int(len(val)//9) # choose a fixed fraction of it
            return OrderedDict((name, out) for name, out in list(odict.items())[:n])
        """
        print("Trying to build the list of top specificity combinations:")
        policy = 3
        for num_foils_chosen in num_foils_chosen_list:
            # top_specificity_single_ordered_guess_combo, top_specificity_single_ordered_guess_value = choose_top_1(get_specificity_from_unsorted_names, num_foils_chosen, list(foil_candidates_dict.keys()))
            # top_specificity_guesses_by_custom_func = choose_custom(get_specificity_from_unsorted_names, num_foils_chosen, custom_policy, list(foil_candidates_dict.keys()))
            perturbing_args= get_specificity_from_unsorted_names, num_foils_chosen, list(foil_candidates_dict.keys()), [0, 1, 2]
            branching_args = get_specificity_from_unsorted_names, num_foils_chosen, policy, list(foil_candidates_dict.keys())
            for method, args, appending_fname in zip((choose_top_n_pretty, perturb_top_1_order), (branching_args, perturbing_args), ("_specificity_by_branching.json", "_specificity_by_perturbing.json")):
                top_specificity_dict = method(*args)
                saving_dict = OrderedDict()
                for key, val in descending_odict(top_specificity_dict).items():
                    str_key = "-".join(key)
                    foil_set = chain_sum([foil_candidates_dict[name] for name in key])
                    saving_dict[str_key] = {"sensitivity-specificity":(get_sensitivity(foil_set.response_per_unit_flux), val), "min melting point":foil_set.get_min_melting_point()}
                    if LIST_PRICE:
                        saving_dict[str_key]["price"] = foil_set.get_price_expr()
                with open(os.path.join(sys.argv[-1], str(num_foils_chosen).zfill(2)+appending_fname), "w") as j:
                    j.write(json.dumps(saving_dict, indent=4))
                break

    if EXAMPLE_UNFOLDINGS:=False:
        # import unfolding module
        sys.path.append("/home/ocean/Documents/GitHubDir/unfoldinggroup/unfolding/unfoldingsuite")
        from datahandler import UnfoldingDataLoader, UnfoldingDataHandlerLite
        from maximumentropy import AMAXED, IMAXED
        from linearleastsquare import PseudoInverse
        from nonlinearleastsquare import GRAVEL
        from collections import defaultdict
        # plotting stuff
        from matplotlib import colors as c
        from matplotlib import cm
        # pick out random combinations of foils, to be reused. Save their names:matrix as dict
        num_apriori_variations, num_foil_combo_samples = 8, 120
        print("Creating a random selection of {} foilsets".format(num_foil_combo_samples))
        sample_foil_combos = {}
        # add in guess min specificity
        min_combo = choose_top_1( negative_get_specificity_from_unsorted_names, num_foils_chosen, list(foil_candidates_dict.keys()) )[0]
        min_combo = tuple(sorted(min_combo))
        sample_foil_combos[min_combo] = chain_sum([foil_candidates_dict[foil] for foil in min_combo]).response_per_unit_flux
        # add in guess max specificity
        max_combo = choose_top_1( get_specificity_from_unsorted_names, num_foils_chosen, list(foil_candidates_dict.keys()) )[0]
        max_combo = tuple(sorted(max_combo))
        sample_foil_combos[max_combo] = chain_sum([foil_candidates_dict[foil] for foil in max_combo]).response_per_unit_flux

        if PERTURB_TOP_AND_BOTTOM_BEST_RANKED:=False:
            for combo in perturb_top_1_order( get_specificity_from_unsorted_names, num_foils_chosen, list(foil_candidates_dict.keys()), skip_evaluation=True).keys():
                sample_foil_combos[combo] = chain_sum([foil_candidates_dict[foil] for foil in combo]).response_per_unit_flux

            for combo in perturb_top_1_order( negative_get_specificity_from_unsorted_names, num_foils_chosen, list(foil_candidates_dict.keys()), skip_evaluation=True).keys():
                sample_foil_combos[combo] = chain_sum([foil_candidates_dict[foil] for foil in combo]).response_per_unit_flux

        while len(sample_foil_combos)<num_foil_combo_samples:
            combo = np.random.choice( list(foil_candidates_dict.keys()), num_foils_chosen, False )
            sample_foil_combos[tuple(sorted(combo))] = chain_sum([foil_candidates_dict[foil] for foil in combo]).response_per_unit_flux

        truth = apriori_fluence.copy()

        D_KL_result, chi2_result, ydev_result = defaultdict(list), defaultdict(list), defaultdict(list)
        algorithms = {"AMAXED":AMAXED, "IMAXED":IMAXED, "PseudoInverse":PseudoInverse, "GRAVEL":GRAVEL}
        desired_chi2 = {"AMAXED":0,    "IMAXED":1E-20,  "PseudoInverse":1E-23, "GRAVEL":1E-5}
        false_aprioris = [false_apriori_generator(truth, i) for i in range(num_apriori_variations)]
        starting_D_KL = [D_KL(secondary_apriori, truth) for secondary_apriori in false_aprioris]
        print("Calcualting the sensitivity and specificity of all foils used.")
        foil_combo_coords = OrderedDict([( name,(get_sensitivity(fset_matrix), get_specificity_lin(fset_matrix)) ) for name, fset_matrix in tqdm(sample_foil_combos.items())])

        possibly_worst= foil_combo_coords[min_combo]
        possibly_best = foil_combo_coords[max_combo]

        for alg_name, alg_type in algorithms.items():
            print("Using algorithm", alg_name, "calculating", num_samples:= len(false_aprioris) * len(sample_foil_combos), "different cases:")
            for secondary_apriori, (name, fset_matrix) in tqdm(itertools.product(false_aprioris, sample_foil_combos.items()), total=num_samples):
                # the rest of the unfolding parameters
                matrix = fset_matrix
                N_meas = matrix @ truth
                sigma_N = np.sqrt(N_meas)

                alg_instance = alg_type(N_meas, matrix, secondary_apriori, sigma_N, desired_chi2=desired_chi2[alg_name], verbosity=0)
                alg_instance.run(alg_type.available_methods[0])
                D_KL_result[name].append(D_KL(alg_instance.solution.phi, truth))
                # chi2_result[name].append(alg_instance.solution.chi2)
                # ydev_result[name].append(alg_instance.solution.phi.sum()/truth.sum() - 1)

            D_KL_wst, D_KL_avg, chi2_wst, chi2_avg, ydev_wst, ydev_avg, ydev_rms = [], [], [], [], [], [], []
            for point_name, point_coord in foil_combo_coords.items():
                D_KL_wst.append( (*point_coord, max(D_KL_result[point_name])) )
                D_KL_avg.append( (*point_coord,mean(D_KL_result[point_name])) )
                # chi2_wst.append( (*point_coord, max(chi2_result[point_name])) )
                # chi2_avg.append( (*point_coord,mean(chi2_result[point_name])) )
                # ydev_wst.append( (*point_coord, max(abs(ydev) for ydev in ydev_result[point_name])) )
                # ydev_avg.append( (*point_coord,mean([abs(ydev) for ydev in ydev_result[point_name]])) )
                # ydev_rms.append( (*point_coord,np.sqrt(mean([ydev**2 for ydev in ydev_result[point_name]]))))
            # D_KL_wst, D_KL_avg, chi2_wst, chi2_avg, ydev_wst, ydev_avg = ary(D_KL_wst), ary(D_KL_avg), ary(chi2_wst), ary(chi2_avg), ary(ydev_wst), ary(ydev_avg) # wrap them into arrays
            if not os.path.exists(os.path.join(sys.argv[-1], PLOT_SAVE_FOLDER)): os.mkdir(os.path.join(sys.argv[-1], PLOT_SAVE_FOLDER))
            # for array, name in zip((D_KL_wst, D_KL_avg, chi2_wst, chi2_avg, ydev_wst, ydev_rms), itertools.product([r"$D_{KL}$", r"$\chi^2$", "relative yield deviation"], ["worst", "average"])):
            for array, name in zip((D_KL_wst, D_KL_avg), ((r"$D_{KL}$","worst"), (r"$D_{KL}$", "average"))):
                x, y, z = ary(array).T
                plt.suptitle("Calculated using the unfolding algorithm {}".format(alg_name))
                plt.title(" ".join(name[::-1])+" of {} different unfolding attempts".format(len(false_aprioris)))
                splot = plt.scatter(x, y, c=z)
                splot.figure.colorbar(cm.ScalarMappable(norm=splot.norm, cmap=splot.cmap))
                x_range = x.max() - x.min()
                plt.xlim(x.min() - x_range*0.1, x.max() + x_range*0.1)
                plt.text(*possibly_best , "possibly best specificity" , va="top", ha="center")
                plt.text(*possibly_worst, "possibly worst specificity", va="top", ha="center")
                plt.xlabel("sensitivity")
                plt.ylabel("specificity")
                plt.savefig(os.path.join(sys.argv[-1], PLOT_SAVE_FOLDER, "_".join(name).replace("$","").replace("{","").replace("}","").replace("^","").replace("\\","").replace(" ","_")
                                                        +"_{}".format(alg_name)+".png"))
                plt.close()
        # record the unfolding results in a defaultdict
        # Take the average of that dictionary
    # save parameters at the end.
    params_dict = dict(SATURATION_COUNT_RATE=SATURATION_COUNT_RATE, COUNT_THRESHOLD=COUNT_THRESHOLD,
                        RANK_BY_DETERMINANT=RANK_BY_DETERMINANT, PLOT_SAVE_FOLDER=PLOT_SAVE_FOLDER, PHYSICAL_PROP_FILE=phy_prop_file_path)
    if LIST_PRICE:
        params_dict.update(dict(PRICE_FILE=PRICE_FILE))
    save_parameters_as_json(sys.argv[-1], params_dict)

    if PLOT_COMPLETE_CURVATURE:=False:
        try:
            f_set = FoilSet(* list(foil_candidates_dict.values()) )
            while True:
                R = f_set.response_per_unit_flux
                S_N_inv = la.inv(np.diag(R @ apriori_fluence))

                # confusion matrix
                sns.heatmap(confusion_matrix(R))
                plt.title(f"Confusion matrix of the foilset with {len(f_set.material_name)} foils")
                plt.show()

                # curvature, presentation 1 (bar plot)
                full_curvature = fractional_curvature_matrix(R, S_N_inv)
                curvature_diag = np.diag(full_curvature)
                plt.bar(range(len(curvature_diag)), curvature_diag)
                plt.suptitle("Curvature of chi2 landscape in the principal directions,")
                plt.title("plotted by bin number")
                plt.yscale('log')
                plt.xlabel("bin number")
                plt.show()

                # curvature, presentation 2 (loglog line plot)
                plt.loglog(gs.flatten(), np.repeat(curvature_diag, 2))
                plt.title("Curvature of chi2 landscape in the principal directions")
                plt.xlabel("E (eV)")
                plt.ylabel("Curvature ((std score)^2/unit fluence(cm^-2))")
                plt.show()

                # curvature, presentation 3 (heatmap)
                sns.heatmap(full_curvature)
                plt.title("Curvature of chi2 landscape")
                plt.show()

                # curvature, presentation 2.5 (incremental)
                for foil_name in f_set.material_name:
                    r_line = foil_candidates_dict[foil_name].response_per_unit_flux
                    S_N_inv = la.inv(np.diag(r_line @ apriori_fluence))
                    curvature_diag = np.diag(fractional_curvature_matrix(r_line, S_N_inv))
                    plt.loglog( gs.flatten(), np.repeat(curvature_diag, 2), label=foil_name)
                plt.loglog(gs.flatten(), np.repeat(curvature_diag, 2), label="Total")
                plt.suptitle("Curvature of the chi2 landscape in the principal directions")
                plt.title("including partial contributions from each")
                plt.legend()
                plt.show()

                # Robin's suggestion
                plt.loglog( (R.T * apriori_fluence).T )
                plt.title("Partial reactionr ates")
                plt.show()

                print("Attempting to pick a second selection ...")
                print("Ignore which of the following foils?")
                print([f_name for f_name in foil_candidates_dict.keys()])
                ignored_foils = input()
                f_set = FoilSet(*[foil for f_name, foil in foil_candidates_dict.items() if f_name not in ignored_foils])

                print("Foils used :\n", "\n".join(f_set.material_name))
        except KeyboardInterrupt:
            print("Terminating...")
    if RANK_BY_DETERMINANT:
        raise NotImplementedError("This feature, while easy to implement, is unlikely to be useful (only useful in very overdetermined case) so it hasn't been implemented yet!")